services:
  training:
    build:
      context: .
      dockerfile: service_model_creation/Dockerfile
    ports:
      - "8080:8080"
    volumes:
      - "training_data:/dossier_host"

  model_runner:
    build:
      context: .
      dockerfile: service_ai_analysis/Dockerfile
    ports:
      - "8081:8081"
    volumes:
      - "training_data:/dossier_host"

  ollama:
    image: ollama/ollama
    container_name: ollama
    volumes:
      - "ollama:/root/.ollama"
    ports:
      - "11434:11434"
    command: ["serve"]
    # Use the following only in a Nvidia GPU environment
    # deploy:
    #   resources:
    #     reservations:
    #       devices:
    #         - driver: nvidia
    #           count: 1
    #           capabilities: [gpu]

volumes:
  ollama:  # Properly define the named volume here
    driver: local
  training_data:
    driver: local
    driver_opts:
      type: none
      device: "./training_data"  # location of the trainign data
      o: bind
